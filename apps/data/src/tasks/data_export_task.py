# Databricks notebook source
# DBTITLE 1,Data Export Task
# Standalone task to export experiment table data in multiple formats (CSV, JSON, Parquet)
# This task runs independently and outputs files to Unity Catalog volumes

# COMMAND ----------

# DBTITLE 1,Imports
import logging
from datetime import datetime
from typing import Optional
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_json, col
from pyspark.sql.types import StructType, ArrayType, MapType
from pyspark.dbutils import DBUtils

# Import openjii utilities
import sys
sys.path.append("/Workspace/Repos/open-jii/apps/data/src/lib/openjii")
from openjii.helpers import load_experiment_table

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# COMMAND ----------

# DBTITLE 1,Task Configuration
# Required parameters
EXPERIMENT_ID = dbutils.widgets.get("EXPERIMENT_ID")
TABLE_NAME = dbutils.widgets.get("TABLE_NAME")
CATALOG_NAME = dbutils.widgets.get("CATALOG_NAME")
FORMAT = dbutils.widgets.get("FORMAT").lower()  # csv, json, or parquet
DOWNLOAD_ID = dbutils.widgets.get("DOWNLOAD_ID")  # UUID generated by the backend
ENVIRONMENT = dbutils.widgets.get("ENVIRONMENT") if dbutils.widgets.get("ENVIRONMENT") else "DEV"

spark = SparkSession.builder.getOrCreate()
dbutils = DBUtils(spark)

# Generate write timestamp
write_time = datetime.now().strftime("%Y%m%d_%H%M%S")

# Output paths
# Structure: /Volumes/{catalog}/centrum/data-downloads/{experiment_id}/{table_name}/{format}/{download_id}/
VOLUME_BASE_PATH = f"/Volumes/{CATALOG_NAME}/centrum/data-downloads"
OUTPUT_BASE_PATH = f"{VOLUME_BASE_PATH}/{EXPERIMENT_ID}/{TABLE_NAME}/{FORMAT}"
OUTPUT_PATH = f"{OUTPUT_BASE_PATH}/{DOWNLOAD_ID}"

logger.info(f"Exporting data for experiment: {EXPERIMENT_ID}")
logger.info(f"Table name: {TABLE_NAME}")
logger.info(f"Format: {FORMAT}")
logger.info(f"Download ID: {DOWNLOAD_ID}")
logger.info(f"Catalog: {CATALOG_NAME}")
logger.info(f"Output path: {OUTPUT_PATH}")

# COMMAND ----------

# DBTITLE 1,Load Data
def load_data():
    """
    Load experiment data using openjii utility.
    This handles all variant parsing and column selection automatically.
    """
    try:
        logger.info(f"Loading experiment data using openjii utility")
        
        # Use openjii utility to load data with proper variant parsing
        df = load_experiment_table(
            experiment_id=EXPERIMENT_ID,
            table_name=TABLE_NAME,
            catalog_name=CATALOG_NAME,
            schema_name="centrum"
        )
        
        row_count = df.count()
        logger.info(f"Loaded {row_count} rows with parsed variants")
        
        if row_count == 0:
            logger.warning("No data found matching the criteria")
        
        return df, row_count
    
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        raise

# COMMAND ----------

# DBTITLE 1,Export Data
def export_data(df):
    """
    Export data in the requested format
    """
    try:
        logger.info(f"Exporting to {FORMAT.upper()}: {OUTPUT_PATH}")
        
        if FORMAT == "csv":
            # CSV doesn't support complex types (structs, arrays, maps)
            # Convert them to JSON strings
            for field in df.schema.fields:
                if isinstance(field.dataType, (StructType, ArrayType, MapType)):
                    df = df.withColumn(field.name, to_json(col(field.name)))
            
            df.write.mode("overwrite").option("header", True).csv(OUTPUT_PATH)
        elif FORMAT == "json":
            df.write.mode("overwrite").json(OUTPUT_PATH)
        elif FORMAT == "parquet":
            df.write.mode("overwrite").parquet(OUTPUT_PATH)
        else:
            raise ValueError(f"Unsupported format: {FORMAT}")
        
        logger.info(f"{FORMAT.upper()} export completed")
        return OUTPUT_PATH
    
    except Exception as e:
        logger.error(f"Error exporting data: {e}")
        raise

# COMMAND ----------

# DBTITLE 1,Get File Path
def get_export_file_path(directory_path):
    """
    Get the actual file path from the export directory
    """
    try:
        # List files in the directory
        files = dbutils.fs.ls(directory_path)
        
        # Filter out metadata files and get actual data files
        data_files = [
            f.path for f in files 
            if not f.name.startswith("_") and not f.name.startswith(".")
        ]
        
        if data_files:
            # For single file output, return the file path
            # For multi-part output, return the directory path
            if len(data_files) == 1:
                logger.info(f"Found single data file")
                return data_files[0]
            else:
                logger.info(f"Found {len(data_files)} data files")
                return directory_path
        else:
            logger.warning(f"No data files found in {directory_path}")
            return directory_path
            
    except Exception as e:
        logger.error(f"Error listing files: {e}")
        return directory_path

# COMMAND ----------

# DBTITLE 1,Main Execution
def main():
    """
    Main execution function
    """
    logger.info("="*80)
    logger.info("Starting data export task")
    logger.info("="*80)
    
    # Load data
    df, row_count = load_data()
    
    if row_count == 0:
        logger.warning("No data to export")
        dbutils.notebook.exit({"status": "no_data", "row_count": 0})
        return
    
    # Export data in requested format
    output_path = export_data(df)
    
    # Get actual file path
    file_path = get_export_file_path(output_path)
    
    # Prepare output
    output = {
        "status": "success",
        "row_count": row_count,
        "experiment_id": EXPERIMENT_ID,
        "table_name": TABLE_NAME,
        "format": FORMAT,
        "download_id": DOWNLOAD_ID,
        "write_time": write_time,
        "file_path": file_path,
    }
    
    logger.info("="*80)
    logger.info("Data export task completed successfully")
    logger.info(f"Total rows exported: {row_count}")
    logger.info(f"Format: {FORMAT.upper()}")
    logger.info(f"File path: {file_path}")
    logger.info("="*80)
    
    # Return results as notebook exit value
    dbutils.notebook.exit(output)

# Run main
main()
